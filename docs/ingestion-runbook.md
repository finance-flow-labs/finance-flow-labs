# Ingestion Runbook

## Core Flow

1. Evaluate source with legal/reliability hard gates.
2. Ingest raw payloads into append-only raw store.
3. Run quality gate checks.
4. Promote passing batches to canonical facts.
5. Route failing batches to quarantine.
6. Serve research queries with point-in-time filters.

## Safety Rules

- Never overwrite historical raw records.
- Enforce `available_at <= decision_time` for replay and backtesting.
- Freeze new source onboarding if monthly budget is exceeded.

## Tier Policy

- Gold: no TTL expiration.
- Silver: 180-day raw cache, 24-month facts.
- Bronze: 30-90 day cache window.

## Manual Update

- SEC run: `python3 -m src.ingestion.cli run-update --source sec_edgar`
- FRED run: `python3 -m src.ingestion.cli run-update --source fred`
- DART run: `python3 -m src.ingestion.cli run-update --source opendart`
- ECOS run: `python3 -m src.ingestion.cli run-update --source ecos`

Environment variables:

- `SUPABASE_DB_URL` or `DATABASE_URL` for run history persistence
- `SEC_USER_AGENT` and optional `SEC_CIK` for SEC source
- `FRED_API_KEY` and optional `FRED_SERIES_ID` for FRED source
- `DART_API_KEY` or `DART_CRTFC_KEY` and optional `DART_CORP_CODE` for DART source
- `ECOS_API_KEY` and optional `ECOS_STAT_CODE` for ECOS source

## Normalization (v1)

- ëª©ì : ì†ŒìŠ¤ë³„ raw payload(FRED/ECOS)ë¥¼ ë¶„ì„ ê³µí†µ í¬ë§·ìœ¼ë¡œ ì •ê·œí™”
- ì •ê·œí™” í¬ì¸íŠ¸ ìŠ¤í‚¤ë§ˆ:
  - `source`, `entity_id`, `metric_key`, `as_of`, `available_at`, `value`, `lineage_id`
- ì €ìž¥ í…Œì´ë¸”: `macro_series_points` (`migrations/004_macro_series_points.sql`)
- Repository API:
  - `write_macro_series_points(points)`
  - `read_macro_series_points(metric_key, limit)`
- ë™ìž‘ ê·œì¹™:
  - `run_ingestion_job()`ì—ì„œ FRED/ECOS ë°°ì¹˜ê°€ canonicalë¡œ ìŠ¹ê²©ë˜ë©´ ì •ê·œí™” í›„ `macro_series_points`ë¥¼ ìžë™ ì ìž¬
  - í’ˆì§ˆ/ì†ŒìŠ¤ ê²Œì´íŠ¸ ì‹¤íŒ¨ë¡œ quarantineëœ ë°°ì¹˜ëŠ” ì •ê·œí™” ì ìž¬ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ

## Macro Analysis Persistence (v1)

- LLM/agent ê¸°ë°˜ ë§¤í¬ë¡œ ë¶„ì„ ê²°ê³¼ ì €ìž¥ í…Œì´ë¸”: `macro_analysis_results`
- ë§ˆì´ê·¸ë ˆì´ì…˜: `migrations/005_macro_analysis_results.sql`
- Repository API:
  - `write_macro_analysis_result(result)`
  - `read_latest_macro_analysis(limit)`
- ì €ìž¥ í•„ë“œì—ëŠ” `regime`, `confidence`, `base/bull/bear`, `policy_case`, `critic_case`, `reason_codes`, `risk_flags`, `triggers`, `narrative`, `model` í¬í•¨

## Forecast Capture (operator-safe)

Use CLI (no direct SQL) to create/update a forecast record with schema validation and idempotency (`thesis_id + horizon + as_of`).

Example:

```bash
python3 -m src.ingestion.cli forecast-record-create \
  --thesis-id thesis-aapl-2026q1 \
  --horizon 1M \
  --expected-return-low 0.03 \
  --expected-return-high 0.09 \
  --expected-volatility 0.20 \
  --expected-drawdown 0.12 \
  --confidence 0.68 \
  --key-drivers-json '["macro:disinflation","sector:semis"]' \
  --evidence-hard-json '[{"source":"fred","metric":"CPI","as_of":"2026-02-20"}]' \
  --evidence-soft-json '[{"source":"news","note":"earnings-call tone improved"}]' \
  --as-of 2026-02-22T00:00:00+00:00
```

Validation guardrails:
- `expected_return_low <= expected_return_high`
- `confidence` must be between `0` and `1`
- `--as-of` must be timezone-aware ISO-8601
- `--evidence-hard-json` must be non-empty JSON array (HARD evidence required)

## Operator Dashboard

- Run: `streamlit run src/dashboard/app.py`
- Required: `SUPABASE_DB_URL` or `DATABASE_URL`
- Dashboard shows:
  - last run status/time
  - raw/canonical/quarantine counters
  - multi-horizon learning metrics with reliability guardrails (`insufficient` / `low_sample` / `reliable`)
    - reliability badges: `ðŸ”´ insufficient`, `ðŸŸ  low_sample`, `ðŸŸ¢ reliable`
    - each row includes a human-readable reliability reason to prevent over-trusting sparse KPI samples
  - recent run history
- Reliability threshold env overrides:
  - `LEARNING_RELIABILITY_MIN_REALIZED_1W` (default: `8`)
  - `LEARNING_RELIABILITY_MIN_REALIZED_1M` (default: `12`)
  - `LEARNING_RELIABILITY_MIN_REALIZED_3M` (default: `6`)
  - `LEARNING_RELIABILITY_COVERAGE_FLOOR` (default: `0.4`)

### Streamlit Community Cloud Deployment

1. Go to `https://share.streamlit.io` and sign in with GitHub.
2. Click `Create app`.
3. Select repository: `finance-flow-labs/finance-flow-labs`.
4. Select branch: `main`.
5. Set Main file path: `streamlit_app.py`.
6. Set Secret: `SUPABASE_DB_URL` (or `DATABASE_URL`).
7. Deploy and copy the generated app URL (`https://<app-name>.streamlit.app`).
